import os
import time 
import gradio as gr
from huggingface_hub import InferenceClient
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
import spaces

"""
For more information on `huggingface_hub` Inference API support, please check the docs: https://huggingface.co/docs/huggingface_hub/v0.22.2/en/guides/inference

NOTE: the base gradio app was autogenerated by HuggingFace spaces 
"""
model_dir = "models"
token = os.getenv('HF_MODEL_TOKEN') 
print("Using token: ", token)

# HF infra baseline
qwen_model_id = "Qwen/Qwen2.5-0.5B-Instruct"
base_client = InferenceClient(qwen_model_id, token=token)

# Local Qwen2.5 variant 
tuned_model_file = "models/1july0732/"
tuned_model = AutoModelForCausalLM.from_pretrained(
    tuned_model_file,
    torch_dtype="auto",
    device_map="auto", 
    local_files_only=True
)
qwen_tokenizer = AutoTokenizer.from_pretrained(qwen_model_id, skip_special_tokens=True)
streamer = TextIteratorStreamer(qwen_tokenizer, skip_prompt=True, skip_special_tokens=True)       

@spaces.GPU
def respond_qwen(message,
    history: list[tuple[str, str]],
    system_message,
    max_tokens,
    temperature
):

    # Two things here... can we omit the explicit tokenizer call and what happens when we change the message 
    # formatting to ... 
    # TODO ensure any completions we're doing are with the help of the GPU! ... test that on HF, we'll need to 
    # move to the sliced GPU setup? ugh
    messages = [ {"role": "system", "content": system_message} ]
    messages.extend(history)
    messages.append({"role": "user", "content": message})

    text = qwen_tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    model_inputs = qwen_tokenizer([text], return_tensors="pt").to(tuned_model.device)

    tuned_model.generate(**model_inputs,max_new_tokens=max_tokens, temperature=temperature, streamer=streamer)
    
    response = ""    
    for text in streamer: 
        if text != "":             
            response += text
            yield response
            # We are synchronous here, but simulate some typing to keep things satisfying. 
            time.sleep(0.02)

# TODO: this inference endpoint periodically has a fit, ironic that the fine-tuned is stable
# replace this with a local load of qwen
@spaces.GPU
def respond_base(
    message,
    history: list[tuple[str, str]],
    system_message,
    max_tokens,
    temperature
):
    messages = [{"role": "system", "content": system_message}]
    messages.extend(history)
    messages.append({"role": "user", "content": message})

    response = ""

    for message in base_client.chat_completion(
        messages,
        max_tokens=max_tokens,
        stream=True,
        temperature=temperature
    ):
        token = message.choices[0].delta.content

        response += token
        yield response

def app(): 
    """ 
    Function to organize Gradio app 
    """
    demo = gr.Blocks()
    with demo: 
        gr.Markdown(value="# ⚙️ Welcome!")
        chat_input = gr.Textbox() 
        with gr.Row():
            with gr.Column(): 
                gr.ChatInterface(
                    title="Qwen2.5-Instruct 0.5b",
                    fn=respond_base,
                    textbox=chat_input, 
                    additional_inputs=[
                        gr.Textbox(value="You are a friendly Chatbot.", label="System message"),
                        gr.Slider(minimum=1, maximum=2048, value=512, step=1, label="Max new tokens"),
                        gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label="Temperature")
                    ],
                    type="messages") 
            with gr.Column(): 
                gr.ChatInterface(
                    title="Qwen2.5-Instruct 0.5b Linux-tuned",
                    fn=respond_qwen,
                    textbox=chat_input, 
                    additional_inputs=[
                        gr.Textbox(value="You are a friendly Chatbot.", label="System message"),
                        gr.Slider(minimum=1, maximum=2048, value=512, step=1, label="Max new tokens"),
                        gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label="Temperature")
                    ],
                    type="messages")         
        demo.launch(share=False)

if __name__ == "__main__":
    app()
