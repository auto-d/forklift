import gradio as gr
import os
from huggingface_hub import InferenceClient
from transformers import AutoModelForCausalLM, OPTForCausalLM, GPT2Tokenizer, TextIteratorStreamer
import torch 

"""
For more information on `huggingface_hub` Inference API support, please check the docs: https://huggingface.co/docs/huggingface_hub/v0.22.2/en/guides/inference

NOTE: the base gradio app was autogenerated by HuggingFace spaces 
"""
model_dir = "models"
token = os.getenv('HF_MODEL_TOKEN') 
print("Using token: ", token)
client = InferenceClient("HuggingFaceH4/zephyr-7b-beta", token=token)

base_model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", torch_dtype=torch.float16)
device = "cuda"
base_model.to(device)

base_tokenizer = GPT2Tokenizer.from_pretrained("facebook/opt-350m")
streamer = TextIteratorStreamer(base_tokenizer)
        
def respond_opt(
    message,
    history: list[tuple[str, str]],
    system_message,
    max_tokens,
    temperature,
    top_p,
):

    input = base_tokenizer([message], return_tensors="pt").to(device)
    ids = base_model.generate(**input, max_new_tokens=100, do_sample=False, streamer=streamer)
    
    response = ""

    for token in streamer: 
        response += token
        yield response

def respond(
    message,
    history: list[tuple[str, str]],
    system_message,
    max_tokens,
    temperature,
    top_p,
):
    messages = [{"role": "system", "content": system_message}]

    for val in history:
        if val[0]:
            messages.append({"role": "user", "content": val[0]})
        if val[1]:
            messages.append({"role": "assistant", "content": val[1]})

    messages.append({"role": "user", "content": message})

    response = ""

    for message in client.chat_completion(
        messages,
        max_tokens=max_tokens,
        stream=True,
        temperature=temperature,
        top_p=top_p,
    ):
        token = message.choices[0].delta.content

        response += token
        yield response


def app(): 
    """ 
    Function to organize Gradio app 
    """
    demo = gr.Blocks()
    with demo: 
        gr.Markdown(value="# ⚙️ Welcome!")
        with gr.Row(): 
            gr.ChatInterface(
                respond_opt,
                additional_inputs=[
                    gr.Textbox(value="You are a friendly Chatbot.", label="System message"),
                    gr.Slider(minimum=1, maximum=2048, value=512, step=1, label="Max new tokens"),
                    gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label="Temperature"),
                    gr.Slider(
                        minimum=0.1,
                        maximum=1.0,
                        value=0.95,
                        step=0.05,
                        label="Top-p (nucleus sampling)",
                    )
                ],
                type="messages") 
            gr.ChatInterface(
                respond,
                additional_inputs=[
                    gr.Textbox(value="You are a friendly Chatbot.", label="System message"),
                    gr.Slider(minimum=1, maximum=2048, value=512, step=1, label="Max new tokens"),
                    gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label="Temperature"),
                    gr.Slider(
                        minimum=0.1,
                        maximum=1.0,
                        value=0.95,
                        step=0.05,
                        label="Top-p (nucleus sampling)",
                    )
                ],
                type="messages")
        
        demo.launch(share=False)

if __name__ == "__main__":
    app()
