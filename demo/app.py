import time 
import gradio as gr
import os
from huggingface_hub import InferenceClient
from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer, TextIteratorStreamer
import torch 

"""
For more information on `huggingface_hub` Inference API support, please check the docs: https://huggingface.co/docs/huggingface_hub/v0.22.2/en/guides/inference

NOTE: the base gradio app was autogenerated by HuggingFace spaces 
"""
model_dir = "models"
token = os.getenv('HF_MODEL_TOKEN') 
print("Using token: ", token)

# HF infra 
client = InferenceClient("Qwen/Qwen2.5-0.5B-Instruct", token=token)

# Local OPT
base_model = OPTForCausalLM.from_pretrained("facebook/opt-350m")
device = "cuda"
base_model.to(device)

base_tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")
streamer = TextIteratorStreamer(base_tokenizer, skip_prompt=True)

# Local Qwen2.5
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen2.5-0.5B-Instruct"
base_model2 = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
base_tokenizer2 = AutoTokenizer.from_pretrained(model_name, skip_special_tokens=True)
streamer2 = TextIteratorStreamer(base_tokenizer2, skip_prompt=True, skip_special_tokens=True)

        
# TODO: something about the way the respond message is being used with both models here is 
# causing feedback ... the model's chats are being reflected back and then we're hitting
# the token generation limit... not ideal. 
def respond_opt(
    message,
    history: list[tuple[str, str]],
    system_message,
    max_tokens,
    temperature,
    top_p,
):
    """
    Gradio ChatInterface callback, which copmlies with the expected footprint and accepts 
    the outputs of a few other Gradio components. From the docs on ChatInterface: 
        Normally (assuming `type` is set to "messages"), the function should accept two parameters: 
        - a `str` representing the input message and 
        -`list` of openai-style dictionaries: 
            {
                "role": "user" | "assistant", 
                "content": `str` | {"path": `str`
            } | `gr.Component`} 
        representing the chat history. The function should return/yield a `str` (for a simple
        message), a supported Gradio component (e.g. gr.Image to return an image), a `dict` 
        (for a complete openai-style message response), or a `list` of such messages.

    """

    input = base_tokenizer([message], return_tensors="pt").to(device)
    ids = base_model.generate(**input, max_new_tokens=100, do_sample=False, streamer=streamer)
    
    response = ""

    for token in streamer: 
        response += token
        yield response

def respond_qwen(message,
    history: list[tuple[str, str]],
    system_message,
    max_tokens,
    temperature,
    top_p,
):

    # Two things here... can we omit the explicit tokenizer call and what happens when we change the message 
    # formatting to ... 
    # TODO ensure any completions we're doing are with the help of the GPU! ... test that on HF, we'll need to 
    # move to the sliced GPU setup? ugh
    messages = [
        {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."},
        {"role": "user", "content": message}
    ]
    text = base_tokenizer2.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    model_inputs = base_tokenizer2([text], return_tensors="pt").to(base_model2.device)

    # streamer argument is explicitly for generated tokens, and will receive each 
    # as it becomes available
    ids = base_model2.generate(**model_inputs,max_new_tokens=512, streamer=streamer2)
    
    response = ""    
    for stream in streamer2: 

        if stream != "": 
            streamed_ids = [
                output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, stream)
            ]
            
            text = base_tokenizer2.decode(streamed_ids, skip_special_tokens=True)[0]
            if text: 
                response += text
                yield response
                sleep(0.02)

def respond(
    message,
    history: list[tuple[str, str]],
    system_message,
    max_tokens,
    temperature,
    top_p,
):
    messages = [{"role": "system", "content": system_message}]

    for val in history:
        if val[0]:
            messages.append({"role": "user", "content": val[0]})
        if val[1]:
            messages.append({"role": "assistant", "content": val[1]})

    messages.append({"role": "user", "content": message})

    response = ""

    for message in client.chat_completion(
        messages,
        max_tokens=max_tokens,
        stream=True,
        temperature=temperature,
        top_p=top_p,
    ):
        token = message.choices[0].delta.content

        response += token
        yield response

def app(): 
    """ 
    Function to organize Gradio app 
    """
    demo = gr.Blocks()
    with demo: 
        gr.Markdown(value="# ⚙️ Welcome!")
        gr.ChatInterface(
            fn=respond_qwen,
            additional_inputs=[
                gr.Textbox(value="You are a friendly Chatbot.", label="System message"),
                gr.Slider(minimum=1, maximum=2048, value=512, step=1, label="Max new tokens"),
                gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label="Temperature"),
                gr.Slider(
                    minimum=0.1,
                    maximum=1.0,
                    value=0.95,
                    step=0.05,
                    label="Top-p (nucleus sampling)",
                )
            ],
            type="messages") 
        
        demo.launch(share=False)

if __name__ == "__main__":
    app()
